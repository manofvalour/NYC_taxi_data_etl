{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb20cf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2929fa7e",
   "metadata": {},
   "source": [
    "## Extract Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b968224c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import io\n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class ExtractData:\n",
    "    def __init__(self,start_date:str, end_date:str):\n",
    "\n",
    "      self.start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "      self.end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "\n",
    "    def extract_nyc_yellow_taxi_data(self,\n",
    "                 url:str =\"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\",):\n",
    "      try:\n",
    "\n",
    "        # Send GET request\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Regex pattern to match High Volume FHV files with date\n",
    "        pattern = re.compile(r\"(yellow_tripdata_)(\\d{4}-\\d{2})\\.parquet\", re.IGNORECASE)\n",
    "\n",
    "        # Loop through all links\n",
    "        download_links = []\n",
    "        for link in soup.find_all(\"a\", href=True):\n",
    "            href = link[\"href\"]\n",
    "            match = pattern.search(href)\n",
    "            if match:\n",
    "                date_str = match.group(2)\n",
    "                file_date = datetime.strptime(date_str, \"%Y-%m\")\n",
    "                if file_date >= self.start_date and file_date <= self.end_date:\n",
    "                    full_url = href if href.startswith(\"http\") else f\"https://www.nyc.gov{href}\"\n",
    "                    download_links.append((date_str, full_url))\n",
    "\n",
    "        # Download and load each file into a DataFrame\n",
    "        yellow_taxi_dfs = pd.DataFrame()\n",
    "        for date_str, link in download_links:\n",
    "            print(f\"Downloading {date_str} from {link}\")\n",
    "            response = requests.get(link)\n",
    "            if response.status_code == 200:\n",
    "                # Convert bytes to file-like object\n",
    "                buffer = io.BytesIO(response.content)\n",
    "                df = pd.read_parquet(buffer)\n",
    "\n",
    "                #adding to dataframe\n",
    "                if yellow_taxi_dfs.empty:\n",
    "                    yellow_taxi_dfs = df\n",
    "                else:\n",
    "                  pd.concat([yellow_taxi_dfs,df])\n",
    "\n",
    "            else:\n",
    "                print(f\"Failed to download {link}\")\n",
    "        print(f\"Data downloaded successfully, {yellow_taxi_dfs.shape}\")\n",
    "        print('\\n')\n",
    "        return yellow_taxi_dfs\n",
    "\n",
    "      except Exception as e:\n",
    "        return (f\"Error: {e}\")\n",
    "\n",
    "    def extract_nyc_network_data(self, API_KEY: List,\n",
    "                                location: str = \"New York NY United States\",\n",
    "                                base_url: str = \"https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline\"):\n",
    "\n",
    "        start_date = self.start_date\n",
    "        end_date = self.end_date\n",
    "\n",
    "        current_date = start_date\n",
    "        full_json = {\n",
    "            \"queryCost\": 0,\n",
    "            \"latitude\": None,\n",
    "            \"longitude\": None,\n",
    "            \"resolvedAddress\": None,\n",
    "            \"address\": location.lower().replace(\" \", \"\"),\n",
    "            \"timezone\": None,\n",
    "            \"tzoffset\": None,\n",
    "            \"description\": None,\n",
    "            \"days\": [],\n",
    "            \"alerts\": [],\n",
    "            \"stations\": {},\n",
    "            \"currentConditions\": {}\n",
    "        }\n",
    "\n",
    "        while current_date <= end_date:\n",
    "            next_month = (current_date.replace(day=28) + timedelta(days=4)).replace(day=1)\n",
    "            month_end = min(next_month - timedelta(days=1), end_date)\n",
    "\n",
    "            start_date_str = current_date.strftime(\"%Y-%m-%d\")\n",
    "            end_date_str = month_end.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "            params = {\n",
    "                \"unitGroup\": \"us\",\n",
    "                \"key\": API_KEY,\n",
    "                \"include\": \"days,hours,current,alerts,stations\",\n",
    "                \"contentType\": \"json\"\n",
    "            }\n",
    "\n",
    "            url = f\"{base_url}/{location}/{start_date_str}/{end_date_str}\"\n",
    "            print(f\"Fetching data from {start_date_str} to {end_date_str}...\")\n",
    "\n",
    "            response = requests.get(url, params=params)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "\n",
    "                # Append daily data\n",
    "                full_json[\"days\"].extend(data.get(\"days\", []))\n",
    "\n",
    "                 # Extract hourly data nested within each day\n",
    "                hourly_records = []\n",
    "                fields = ['datetime', 'temp', 'dew', 'humidity', 'precip', 'preciptype', 'snow', 'snowdepth', 'visibility']\n",
    "                for day in full_json.get(\"days\", []):\n",
    "                    for hour in day.get(\"hours\", []):\n",
    "                      filtered_hour = {key: hour.get(key) for key in fields}\n",
    "                      filtered_hour[\"day\"] = day.get(\"datetime\")  # Add parent day for context\n",
    "                      hourly_records.append(filtered_hour)\n",
    "\n",
    "                ## create a pandas dataframe\n",
    "                df_hours = pd.DataFrame(hourly_records)\n",
    "\n",
    "                print(f\"Retrieved {len(data.get('days', []))} days.\")\n",
    "            elif response.status_code == 429:\n",
    "                print(f\" Rate limit hit ({response.status_code}): {response.text}\")\n",
    "            else:\n",
    "                print(f\"Error {response.status_code}: {response.text}\")\n",
    "\n",
    "            current_date = next_month\n",
    "            time.sleep(3)\n",
    "\n",
    "        print(f\"\\n Retrieved data contain {df_hours.shape[0]//24} days of data.\")\n",
    "        return df_hours\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cd4ed5",
   "metadata": {},
   "source": [
    "## Transform Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d2bf485",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self,nyc_taxi_data:pd.DataFrame, nyc_weather_data:pd.DataFrame):\n",
    "        self.nyc_taxi_data = nyc_taxi_data\n",
    "        self.nyc_weather_data = nyc_weather_data\n",
    "\n",
    "    def transform_nyc_yellow_taxi_data(self):\n",
    "      nyc_taxi_data = self.nyc_taxi_data\n",
    "\n",
    "      # Converting the VendorID\n",
    "      vendor_id= {1: \"Creative Mobile Technologies, LLC\",\n",
    "                  2: \"Curb Mobility, LLC\",\n",
    "                  6: 'Myle Technologies Inc',\n",
    "                  7: 'Helix'\n",
    "               }\n",
    "      nyc_taxi_data['VendorID'] = nyc_taxi_data['VendorID'].map(vendor_id)\n",
    "\n",
    "      # Handling the RateCard\n",
    "      ratecode= {1: \"Standard rate\",\n",
    "                 2: \"JFK\",\n",
    "                 3: \"Newark\",\n",
    "                 4: \"Nassau or Westchester\",\n",
    "                 5: \"Negotiated fare\",\n",
    "                 6: \"Group ride\",\n",
    "                 99: 'Null/unknown'\n",
    "              }\n",
    "      nyc_taxi_data['RatecodeID'] = nyc_taxi_data['RatecodeID'].map(ratecode)\n",
    "\n",
    "      ## Handling store and fwd flag\n",
    "      store_and_forward= {\n",
    "          'N': \"No\",\n",
    "          'Y': \"Yes\"\n",
    "      }\n",
    "      nyc_taxi_data['store_and_fwd_flag'] = nyc_taxi_data['store_and_fwd_flag'].map(store_and_forward)\n",
    "\n",
    "      ## Handling Payment type\n",
    "      payment_type= {\n",
    "          0: 'Flex Fair Trip',\n",
    "          1: \"Credit card\",\n",
    "          2: \"Cash\",\n",
    "          3: \"No charge\",\n",
    "          4: \"Dispute\",\n",
    "          5: \"Unknown\",\n",
    "          6: \"Voided trip\"\n",
    "      }\n",
    "      nyc_taxi_data['payment_type'] = nyc_taxi_data['payment_type'].map(payment_type)\n",
    "\n",
    "      # Convert tpep_pickup_datetime and tpep_dropoff_datetime to datetime\n",
    "      nyc_taxi_data['pickup_datetime'] = pd.to_datetime(nyc_taxi_data['tpep_pickup_datetime'])\n",
    "      nyc_taxi_data['dropoff_datetime'] = pd.to_datetime(nyc_taxi_data['tpep_dropoff_datetime'])\n",
    "\n",
    "      ## handling the Pulocation_Id and the DOLocation_Id\n",
    "      lookup_table = pd.read_csv(\"https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\") ## importing the lookup table\n",
    "      nyc_taxi_data = nyc_taxi_data.merge(lookup_table, left_on='PULocationID', right_on='LocationID', )\n",
    "      nyc_taxi_data = nyc_taxi_data.merge(lookup_table, left_on='DOLocationID', right_on='LocationID')\n",
    "\n",
    "      ## dropping the columns\n",
    "      nyc_taxi_data.drop(columns=['PULocationID', 'DOLocationID'], inplace=True)\n",
    "      nyc_taxi_data.drop(columns=['tpep_pickup_datetime', 'tpep_dropoff_datetime'], inplace=True)\n",
    "      nyc_taxi_data.drop(columns=['LocationID_x', 'LocationID_y'], inplace=True)\n",
    "\n",
    "      ##renaming columns\n",
    "      nyc_taxi_data.rename(columns={'Zone_x': 'PULocation_zone', 'Zone_y': 'DOLocation_zone'}, inplace=True)\n",
    "      nyc_taxi_data.rename(columns={'Borough_x': 'PULocation_borough', 'Borough_y': 'DOLocation_borough'}, inplace=True)\n",
    "      nyc_taxi_data.rename(columns={'service_zone_x': 'PULocation_service_zone', 'service_zone_y': 'DOLocation_service_zone'}, inplace=True)\n",
    "\n",
    "      return nyc_taxi_data\n",
    "\n",
    "    def transform_weather_data(self):\n",
    "      nyc_weather_data = self.nyc_weather_data\n",
    "\n",
    "      ## changing the datatype of datetime column to datetime\n",
    "      nyc_weather_data['datetime'] = pd.to_datetime(nyc_weather_data['day'].astype(str) + ' ' + nyc_weather_data['datetime'].astype(str))\n",
    "      nyc_weather_data.drop(columns=['day'], inplace=True)\n",
    "\n",
    "\n",
    "      return nyc_weather_data\n",
    "\n",
    "\n",
    "    def merge_data(self):\n",
    "      taxi_data = self.transform_nyc_yellow_taxi_data()\n",
    "      weather_data = self.transform_weather_data()\n",
    "\n",
    "      ## merging the taxi data and the weather data together\n",
    "      taxi_data['new_pickup_datetime'] = taxi_data['pickup_datetime'].dt.round('h')\n",
    "      transformed_data = pd.merge(taxi_data, weather_data, left_on=['new_pickup_datetime'] , right_on=['datetime'], how='left')\n",
    "      transformed_data.drop(columns=['new_pickup_datetime','datetime'], inplace=True)\n",
    "\n",
    "      return transformed_data\n",
    "\n",
    "    def transform(self):\n",
    "      return self.merge_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6badf14",
   "metadata": {},
   "source": [
    "## Load Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a006272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "class LoadData:\n",
    "    def __init__(self, transformed_data: pd.DataFrame, file_name: str):\n",
    "        self.transformed_data = transformed_data\n",
    "        self.file_name = file_name\n",
    "\n",
    "    def save(self):\n",
    "        try:\n",
    "            if not os.path.exists(self.file_name) or os.path.getsize(self.file_name) == 0:\n",
    "                self.transformed_data.to_parquet(self.file_name, index=False)\n",
    "                print('✅ Data saved successfully (new file or empty file)')\n",
    "            else:\n",
    "                existing_data = pd.read_parquet(self.file_name)\n",
    "                combined_data = pd.concat([existing_data, self.transformed_data]).drop_duplicates()\n",
    "                combined_data.to_parquet(self.file_name, index=False)\n",
    "                print('✅ Data appended successfully')\n",
    "\n",
    "            return self.transformed_data.shape\n",
    "\n",
    "        except Exception as e:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60d88b2",
   "metadata": {},
   "source": [
    "## ETL Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ad8a174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class ETLPipeline:\n",
    "    def __init__(self, start_date: str, end_date: str, file_name: str):\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.file_name = file_name\n",
    "\n",
    "    def run(self, api_key:str):\n",
    "        # Extract\n",
    "        print('Extracting data...')\n",
    "\n",
    "        extract = ExtractData(self.start_date, self.end_date)\n",
    "        nyc_taxi_data = extract.extract_nyc_yellow_taxi_data()\n",
    "        nyc_weather_data = extract.extract_nyc_network_data(api_key)\n",
    "\n",
    "        #Transform\n",
    "        transformer = DataTransformation(nyc_taxi_data, nyc_weather_data)\n",
    "        transformed_data = transformer.transform()\n",
    "\n",
    "        # Load\n",
    "        load = LoadData(transformed_data, self.file_name)\n",
    "        load.save()\n",
    "        print(f'data saved to {self.file_name}')\n",
    "        print(f'data shape: {load.save()}')\n",
    "        print('\\n')\n",
    "        print('✅ ETL pipeline completed successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a94af8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
