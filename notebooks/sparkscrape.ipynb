{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ef87e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import io\n",
    "import time\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ExtractData:\n",
    "    def __init__(self,start_date:str, end_date:str):\n",
    "\n",
    "      self.start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "      self.end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "      self.spark = SparkSession.builder.appName('NYC_taxi').getOrCreate()\n",
    "\n",
    "\n",
    "    def extract_nyc_yellow_taxi_data(self,\n",
    "                 url:str =\"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\",):\n",
    "      try:\n",
    "\n",
    "        # Send GET request\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Regex pattern to match High Volume FHV files with date\n",
    "        pattern = re.compile(r\"(yellow_tripdata_)(\\d{4}-\\d{2})\\.parquet\", re.IGNORECASE)\n",
    "\n",
    "        # Loop through all links\n",
    "        download_links = []\n",
    "        for link in soup.find_all(\"a\", href=True):\n",
    "            href = link[\"href\"]\n",
    "            match = pattern.search(href)\n",
    "            if match:\n",
    "                date_str = match.group(2)\n",
    "                file_date = datetime.strptime(date_str, \"%Y-%m\")\n",
    "                if file_date >= self.start_date and file_date <= self.end_date:\n",
    "                    full_url = href if href.startswith(\"http\") else f\"https://www.nyc.gov{href}\"\n",
    "                    download_links.append((date_str, full_url))\n",
    "\n",
    "        # Download and load each file into a DataFrame\n",
    "        yellow_taxi_dfs = None\n",
    "\n",
    "        for date_str, link in download_links:\n",
    "            print(f\"Downloading {date_str} from {link}\")\n",
    "            response = requests.get(link)\n",
    "            if response.status_code == 200:\n",
    "                # Saving file in a tmp filepath\n",
    "\n",
    "                with tempfile.NamedTemporaryFile(delete=False, suffix=\".parquet\") as tmp_file:\n",
    "                    tmp_file.write(response.content)\n",
    "                    tmp_path = tmp_file.name\n",
    "\n",
    "                df = self.spark.read.parquet(tmp_path)\n",
    "\n",
    "                #adding to dataframe\n",
    "                if yellow_taxi_dfs is None:\n",
    "                    yellow_taxi_dfs = df\n",
    "                else:\n",
    "                  yellow_taxi_dfs=yellow_taxi_dfs.union(df)\n",
    "\n",
    "            else:\n",
    "                print(f\"Failed to download {link}\")\n",
    "\n",
    "        if yellow_taxi_dfs:\n",
    "          print(f\"Data downloaded successfully: {yellow_taxi_dfs.count()} rows, {len(yellow_taxi_dfs.columns)} columns\")\n",
    "          return yellow_taxi_dfs\n",
    "        \n",
    "        else:\n",
    "          print(\"No data found for the given date range.\")\n",
    "          return None\n",
    "\n",
    "      except Exception as e:\n",
    "        return (f\"Error: {e}\")\n",
    "      \n",
    "\n",
    "    def extract_nyc_network_data(self, API_KEY: str,\n",
    "                                location: str = \"New York NY United States\",\n",
    "                                base_url: str = \"https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline\"):\n",
    "\n",
    "        start_date = self.start_date\n",
    "        end_date = self.end_date\n",
    "\n",
    "        current_date = start_date\n",
    "\n",
    "        all_hourly_records = []\n",
    "        metadata_captured = False\n",
    "        full_json = {}\n",
    "\n",
    "        while current_date <= end_date:\n",
    "            next_month = (current_date.replace(day=28) + timedelta(days=4)).replace(day=1)\n",
    "            month_end = min(next_month - timedelta(days=1), end_date)\n",
    "\n",
    "            start_date_str = current_date.strftime(\"%Y-%m-%d\")\n",
    "            end_date_str = month_end.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "            params = {\n",
    "                \"unitGroup\": \"us\",\n",
    "                \"key\": API_KEY,\n",
    "                \"include\": \"days,hours,current,alerts,stations\",\n",
    "                \"contentType\": \"json\"\n",
    "            }\n",
    "\n",
    "            url = f\"{base_url}/{location}/{start_date_str}/{end_date_str}\"\n",
    "            print(f\"Fetching data from {start_date_str} to {end_date_str}...\")\n",
    "\n",
    "            response = requests.get(url, params=params)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "\n",
    "                # Capture Metadata once\n",
    "                if not metadata_captured:\n",
    "                  full_json.update({k: data.get(k) for k in [\n",
    "                      \"queryCost\", \"latitude\", \"longitude\", \"resolvedAddress\",\n",
    "                      \"address\", \"timezone\", \"tzoffset\", \"description\"\n",
    "                  ]})\n",
    "                  metadata_captured = True\n",
    "\n",
    "                # Extract hourly data for this month only\n",
    "                fields = ['datetime', 'temp', 'dew', 'humidity', 'precip', 'preciptype', 'snow', 'snowdepth', 'visibility']\n",
    "                for day in data.get(\"days\", []):\n",
    "                    for hour in day.get(\"hours\", []):\n",
    "                        filtered_hour = {key: hour.get(key) for key in fields}\n",
    "                        filtered_hour[\"day\"] = day.get(\"datetime\")\n",
    "                        all_hourly_records.append(filtered_hour)\n",
    "\n",
    "                print(f\"Retrieved {len(data.get('days', []))} days.\")\n",
    "\n",
    "            elif response.status_code == 429:\n",
    "                print(f\" Rate limit hit ({response.status_code}): {response.text}\")\n",
    "            else:\n",
    "                print(f\"Error {response.status_code}: {response.text}\")\n",
    "\n",
    "            current_date = next_month\n",
    "            time.sleep(3)\n",
    "\n",
    "        if all_hourly_records:\n",
    "          # Create Spark DataFrame from list of dicts\n",
    "          df_hours = self.spark.createDataFrame(all_hourly_records)\n",
    "\n",
    "          # Count rows and estimate days (assuming 24 hourly records per day)\n",
    "          total_rows = df_hours.count()\n",
    "          print(f\"\\nRetrieved data contains {total_rows // 24} days of data.\")\n",
    "\n",
    "          return df_hours\n",
    "        else:\n",
    "          print(\"No data retrieved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp, concat_ws, round as spark_round\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, nyc_taxi_data: DataFrame, nyc_weather_data: DataFrame, spark: SparkSession):\n",
    "        self.nyc_taxi_data = nyc_taxi_data\n",
    "        self.nyc_weather_data = nyc_weather_data\n",
    "        self.spark = spark\n",
    "\n",
    "    def transform_nyc_yellow_taxi_data(self) -> DataFrame:\n",
    "        df = self.nyc_taxi_data\n",
    "\n",
    "        # VendorID mapping\n",
    "        vendor_id = {\n",
    "            1: \"Creative Mobile Technologies, LLC\",\n",
    "            2: \"Curb Mobility, LLC\",\n",
    "            6: \"Myle Technologies Inc\",\n",
    "            7: \"Helix\"\n",
    "        }\n",
    "        df = df.replace(vendor_id, subset=[\"VendorID\"])\n",
    "\n",
    "        # RatecodeID mapping\n",
    "        ratecode = {\n",
    "            1: \"Standard rate\",\n",
    "            2: \"JFK\",\n",
    "            3: \"Newark\",\n",
    "            4: \"Nassau or Westchester\",\n",
    "            5: \"Negotiated fare\",\n",
    "            6: \"Group ride\",\n",
    "            99: \"Null/unknown\"\n",
    "        }\n",
    "        df = df.replace(ratecode, subset=[\"RatecodeID\"])\n",
    "\n",
    "        # Store and forward flag mapping\n",
    "        store_and_forward = {\n",
    "            \"N\": \"No\",\n",
    "            \"Y\": \"Yes\"\n",
    "        }\n",
    "        df = df.replace(store_and_forward, subset=[\"store_and_fwd_flag\"])\n",
    "\n",
    "        # Payment type mapping\n",
    "        payment_type = {\n",
    "            0: \"Flex Fair Trip\",\n",
    "            1: \"Credit card\",\n",
    "            2: \"Cash\",\n",
    "            3: \"No charge\",\n",
    "            4: \"Dispute\",\n",
    "            5: \"Unknown\",\n",
    "            6: \"Voided trip\"\n",
    "        }\n",
    "        df = df.replace(payment_type, subset=[\"payment_type\"])\n",
    "\n",
    "        # Convert pickup and dropoff datetime\n",
    "        df = df.withColumn(\"pickup_datetime\", to_timestamp(col(\"tpep_pickup_datetime\"))) \\\n",
    "               .withColumn(\"dropoff_datetime\", to_timestamp(col(\"tpep_dropoff_datetime\")))\n",
    "\n",
    "        # Load lookup table into Spark\n",
    "        lookup_table = self.spark.read.csv(\n",
    "            \"https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\",\n",
    "            header=True,\n",
    "            inferSchema=True\n",
    "        )\n",
    "\n",
    "        # Join for pickup location\n",
    "        df = df.join(\n",
    "            lookup_table.withColumnRenamed(\"LocationID\", \"PULocationID_lookup\")\n",
    "                        .withColumnRenamed(\"Borough\", \"PULocation_borough\")\n",
    "                        .withColumnRenamed(\"Zone\", \"PULocation_zone\")\n",
    "                        .withColumnRenamed(\"service_zone\", \"PULocation_service_zone\"),\n",
    "            df.PULocationID == col(\"PULocationID_lookup\"),\n",
    "            \"left\"\n",
    "        )\n",
    "\n",
    "        # Join for dropoff location\n",
    "        df = df.join(\n",
    "            lookup_table.withColumnRenamed(\"LocationID\", \"DOLocationID_lookup\")\n",
    "                        .withColumnRenamed(\"Borough\", \"DOLocation_borough\")\n",
    "                        .withColumnRenamed(\"Zone\", \"DOLocation_zone\")\n",
    "                        .withColumnRenamed(\"service_zone\", \"DOLocation_service_zone\"),\n",
    "            df.DOLocationID == col(\"DOLocationID_lookup\"),\n",
    "            \"left\"\n",
    "        )\n",
    "\n",
    "        # Drop unnecessary columns\n",
    "        df = df.drop(\"PULocationID\", \"DOLocationID\",\n",
    "                     \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\",\n",
    "                     \"PULocationID_lookup\", \"DOLocationID_lookup\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def transform_weather_data(self) -> DataFrame:\n",
    "        df = self.nyc_weather_data\n",
    "\n",
    "        # Combine day + datetime into one timestamp\n",
    "        df = df.withColumn(\n",
    "            \"datetime\",\n",
    "            to_timestamp(concat_ws(\" \", col(\"day\"), col(\"datetime\")))\n",
    "        ).drop(\"day\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def merge_data(self) -> DataFrame:\n",
    "        taxi_df = self.transform_nyc_yellow_taxi_data()\n",
    "        weather_df = self.transform_weather_data()\n",
    "\n",
    "        # Round pickup time to nearest hour\n",
    "        taxi_df = taxi_df.withColumn(\"new_pickup_datetime\", spark_round(col(\"pickup_datetime\").cast(\"double\") / 3600) * 3600)\n",
    "        taxi_df = taxi_df.withColumn(\"new_pickup_datetime\", to_timestamp(col(\"new_pickup_datetime\")))\n",
    "\n",
    "        # Join on datetime\n",
    "        merged_df = taxi_df.join(\n",
    "            weather_df,\n",
    "            taxi_df.new_pickup_datetime == weather_df.datetime,\n",
    "            \"left\"\n",
    "        ).drop(\"new_pickup_datetime\", \"datetime\")\n",
    "\n",
    "        return merged_df\n",
    "\n",
    "    def transform(self) -> DataFrame:\n",
    "        return self.merge_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "\n",
    "class LoadData:\n",
    "    def __init__(self, transformed_data: DataFrame, file_name: str, spark: SparkSession):\n",
    "        self.transformed_data = transformed_data\n",
    "        self.file_name = file_name\n",
    "        self.spark = spark\n",
    "\n",
    "    def save(self):\n",
    "        try:\n",
    "            if not os.path.exists(self.file_name) or len(os.listdir(self.file_name)) == 0:\n",
    "                # Save as new Parquet dataset\n",
    "                self.transformed_data.write.mode(\"overwrite\").parquet(self.file_name)\n",
    "                print(\"✅ Data saved successfully (new file or empty file)\")\n",
    "            else:\n",
    "                # Read existing Parquet dataset\n",
    "                existing_data = self.spark.read.parquet(self.file_name)\n",
    "\n",
    "                # Union and drop duplicates\n",
    "                combined_data = existing_data.unionByName(self.transformed_data).dropDuplicates()\n",
    "\n",
    "                # Overwrite with combined data\n",
    "                combined_data.write.mode(\"overwrite\").parquet(self.file_name)\n",
    "                print(\"✅ Data appended successfully\")\n",
    "\n",
    "            return (self.transformed_data.count(), len(self.transformed_data.columns))\n",
    "\n",
    "        except Exception as e:\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf64ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.NYC_taxi_data_pipeline.utils.custom_utils import read_yaml\n",
    "from src.NYC_taxi_data_pipeline.constants import *\n",
    "\n",
    "DATA = read_yaml(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA.DATA_EXTRACTION_CONFIG.Artifact_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv('API_KEY')\n",
    "\n",
    "print(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f099bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddba588",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
